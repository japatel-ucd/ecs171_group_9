{"cells":[{"cell_type":"markdown","metadata":{"id":"-bN1_O5oJDkj"},"source":["*Code modified from https://keras.io/examples/vision/swin_transformers/*\n","\n","*Keras/Tensorflow elements have been converted into Pytorch*"]},{"cell_type":"markdown","metadata":{"id":"qoALdllmxDp0"},"source":["### Stuff to import (can add to this as needed)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yFefuqTtw-NL","executionInfo":{"status":"ok","timestamp":1654278161605,"user_tz":420,"elapsed":139,"user":{"displayName":"Donald L Chan","userId":"14677021687740246131"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision import transforms"]},{"cell_type":"markdown","metadata":{"id":"ScBLuYliwFQK"},"source":["### Data preparation"]},{"cell_type":"markdown","metadata":{"id":"amucVe3wkFPr"},"source":["##### Helper Code"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NlbmK2QMB0ry","executionInfo":{"status":"ok","timestamp":1654278166077,"user_tz":420,"elapsed":364,"user":{"displayName":"Donald L Chan","userId":"14677021687740246131"}}},"outputs":[],"source":["# libraries necessary\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# for setting up the file structure necessary for PyTorch\n","def copy_images():\n","    image_path = './images/'\n","\n","    image_dir = np.array(glob(\"./images/train/*\"))\n","\n","    count = 0\n","    for file in image_dir:\n","        count = count + 1\n","        if count % 5000 == 0 or count == 1:\n","            print(\"TRAIN IMAGES MOVED:\", count)\n","                \n","        new_file = file.split('\\\\')[-1]\n","        category, new_file = new_file.split('_', 1)\n","        category_path = image_path + 'swin/' + 'train/' + category\n","        if not os.path.exists(category_path):\n","            os.makedirs(category_path)\n","        new_file = category_path + '/' + new_file\n","        if os.path.exists(new_file): continue\n","        shutil.copyfile(file, new_file)\n","\n","\n","    image_dir = np.array(glob(\"./images/test/*\"))\n","\n","    count = 0\n","    for file in image_dir:\n","        count = count + 1\n","        if count % 100 == 0 or count == 1:\n","            print(\"TEST IMAGES MOVED:\", count)\n","                \n","        new_file = file.split('\\\\')[-1]\n","        category, new_file = new_file.split('_', 1)\n","        category_path = image_path + 'swin/' + 'test/' + category\n","        if not os.path.exists(category_path):\n","            os.makedirs(category_path)\n","        new_file = category_path + '/' + new_file\n","        if os.path.exists(new_file): continue\n","        shutil.copyfile(file, new_file)\n","\n","# for displaying PyTorch images\n","def imshow(image, ax=None, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    if ax is None:\n","        fig, ax = plt.subplots()\n","    image = image.numpy().transpose((1, 2, 0))\n","\n","    ax.imshow(image)\n","    ax.spines['top'].set_visible(False)\n","    ax.spines['right'].set_visible(False)\n","    ax.spines['left'].set_visible(False)\n","    ax.spines['bottom'].set_visible(False)\n","    ax.tick_params(axis='both', length=0)\n","    ax.set_xticklabels('')\n","    ax.set_yticklabels('')\n","    ax.set_title(title)\n","\n","    return ax\n","\n","# Enumeration for classes\n","img_labels = {\n","    0: 'de',\n","    1: 'en',\n","}\n","\n","### Need to specify num. of classes and input shape ###\n","num_classes = 2\n","input_shape = (64, 64)\n","\n","# for creating the train/validation sets\n","def split_dataset(data_dir, split_size):\n","    if split_size == None or split_size <= 0.0:\n","        print(\"Invalid validation value\")\n","        return\n","    \n","    train_transform = transforms.Compose([transforms.ToTensor()])\n","    valid_transform = transforms.Compose([transforms.ToTensor()])\n","    \n","    train_data = datasets.ImageFolder(data_dir + '/train/', transform = train_transform)\n","    valid_data = datasets.ImageFolder(data_dir + '/train/', transform = train_transform)\n","\n","    num_train = len(train_data)\n","    indices = list(range(num_train))\n","    split = int(np.floor(split_size * num_train))\n","    np.random.shuffle(indices)\n","    \n","    train_idx, valid_idx = indices[split:], indices[:split]\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","    validloader = torch.utils.data.DataLoader(valid_data, sampler=valid_sampler, batch_size=32)\n","    \n","    return trainloader, validloader"]},{"cell_type":"markdown","metadata":{"id":"QaF3arM5klpy"},"source":["##### Loading dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"XfWyhenpklDF","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1654278401358,"user_tz":420,"elapsed":137,"user":{"displayName":"Donald L Chan","userId":"14677021687740246131"}},"outputId":"97f5740c-6f16-47dd-a480-1f5c27eaba44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying images\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-60b85a06300a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-ed60914aa56d>\u001b[0m in \u001b[0;36msplit_dataset\u001b[0;34m(data_dir, split_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mvalid_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     ) -> None:\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/train/'"]}],"source":["if not os.path.exists('./dataset'):\n","    print(\"Copying images\")\n","    copy_images()\n","\n","data_dir = './dataset'\n","\n","trainloader, validloader = split_dataset(data_dir, 0.2)\n","\n","test_transform = transforms.Compose([transforms.ToTensor()])\n","test_data = datasets.ImageFolder(data_dir + '/test/', transform = test_transform)\n","testloader = torch.utils.data.DataLoader(test_data)\n","\n","print(\"Train size:\", len(trainloader))\n","print(\"Validation size:\", len(validloader))\n","print(\"Test size:\", len(testloader))\n","\n","data_iter = iter(trainloader)\n","\n","images, labels = next(data_iter)\n","\n","fig, axes = plt.subplots(figsize = (10,4), ncols = 3)\n","\n","for i in range(3):\n","    \n","    ax = axes[i]\n","    label = img_labels[labels[i].item()]\n","    imshow(images[i], ax, label)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"HVgmsKtBwKzf"},"source":["### Hyperparameter configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLpfMfTfwNku"},"outputs":[],"source":["patch_size = (2, 2)   # 2-by-2 sized patches\n","dropout_rate = 0.03   # Dropout rate\n","num_heads = 8         # Attention heads\n","embed_dim = 64        # Embedding dimension\n","num_mlp = 256         # MLP layer size\n","qkv_bias = True       # Convert embedded patches to query, key, and values with a learnable additive value\n","window_size = 2       # Size of attention window\n","shift_size = 1        # Size of shifting window\n","image_dimension = 64  # Initial image size\n","\n","num_patch_x = input_shape[0] // patch_size[0]\n","num_patch_y = input_shape[1] // patch_size[1]\n","\n","\n","#test these\n","learning_rate = 1e-3\n","batch_size = 128\n","num_epochs = 100\n","validation_split = 0.1\n","weight_decay = 0.0001\n","label_smoothing = 0.1"]},{"cell_type":"markdown","metadata":{"id":"qkjHOD7EwN8Y"},"source":["### Helper Functions\n","(Extract sequence of patches from the image, merge patches, and apply dropout)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRGoMGVAwRk_"},"outputs":[],"source":["def window_partition(x, window_size):\n","    _, height, width, channels = x.shape\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = torch.reshape(\n","        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n","    )\n","    x = torch.permute(x, (0, 1, 3, 2, 4, 5))\n","    windows = torch.reshape(x, shape=(-1, window_size, window_size, channels))\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, height, width, channels):\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = torch.reshape(\n","        windows,\n","        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n","    )\n","    x = torch.permute(x, (0, 1, 3, 2, 4, 5))\n","    x = torch.reshape(x, shape=(-1, height, width, channels))\n","    return x\n","\n","\n","class DropPath(nn.Module):\n","    def __init__(self, drop_prob=None, **kwargs):\n","        super(DropPath, self).__init__(**kwargs)\n","        self.drop_prob = drop_prob\n","\n","    def call(self, x):\n","        input_shape = list(torch.size(x))\n","        batch_size = input_shape[0]\n","        rank = x.shape.rank\n","\n","        rank = len(list(torch.size(x)))\n","        shape = (batch_size,) + (1,) * (rank - 1)\n","        random_tensor = (1 - self.drop_prob) + torch.rand(shape, dtype=x.dtype)\n","        path_mask = torch.floor(random_tensor)\n","        output = torch.div(x, 1 - self.drop_prob) * path_mask\n","        return output\n"]},{"cell_type":"markdown","metadata":{"id":"mwhZgrahwSoY"},"source":["### Window based multi-head self-attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BsJcTd9twW4i"},"outputs":[],"source":["class WindowAttention(nn.Module):\n","  def __init__(\n","      self, \n","      dim, \n","      window_size, \n","      num_heads, \n","      qkv_bias=True, \n","      dropout_rate=0.0, \n","      **kwargs\n","  ):\n","      super(WindowAttention, self).__init__(**kwargs)\n","      self.dim = dim\n","      self.window_size = window_size\n","      self.num_heads = num_heads\n","      self.scale = (dim // num_heads) ** -0.5\n","      self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","      self.dropout = nn.Dropout(dropout_rate)\n","      self.proj = nn.Linear(dim * 3, dim)\n","\n","  def build(self, input_shape):\n","      num_window_elements = (2 * self.window_size[0] - 1) * (\n","          2 * self.window_size[1] - 1\n","      )\n","      self.relative_position_bias_table = nn.Parameter(torch.zeros(\n","          num_window_elements, self.num_heads\n","      ))\n","      coords_h = np.arange(self.window_size[0])\n","      coords_w = np.arange(self.window_size[1])\n","      coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n","      coords = np.stack(coords_matrix)\n","      coords_flatten = coords.reshape(2, -1)\n","      relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","      relative_coords = relative_coords.transpose([1, 2, 0])\n","      relative_coords[:, :, 0] += self.window_size[0] - 1\n","      relative_coords[:, :, 1] += self.window_size[1] - 1\n","      relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","      relative_position_index = relative_coords.sum(-1)\n","\n","      self.relative_position_index = Variable(torch.tensor(relative_position_index), autograd=False)\n","\n","  def call(self, x, mask=None):\n","        _, size, channels = x.shape\n","        head_dim = channels // self.num_heads\n","        x_qkv = self.qkv(x)\n","        x_qkv = torch.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n","        x_qkv = torch.permute(x_qkv, dims=(2, 0, 3, 1, 4))\n","        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n","        q = q * self.scale\n","        k = torch.permute(k, (0, 1, 3, 2))\n","        attn = q @ k\n","\n","        num_window_elements = self.window_size[0] * self.window_size[1]\n","        relative_position_index_flat = torch.reshape(\n","            self.relative_position_index, shape=(-1,)\n","        )\n","        relative_position_bias = torch.gather(\n","            self.relative_position_bias_table, dims=relative_position_index_flat\n","        )\n","        relative_position_bias = torch.reshape(\n","            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n","        )\n","        relative_position_bias = torch.permute(relative_position_bias, dims=(2, 0, 1))\n","        attn = attn + torch.unsqueeze(relative_position_bias, dim=0)\n","\n","        if mask is not None:\n","            nW = mask.size()[0]\n","            mask_float = torch.unsqueeze(\n","                torch.unsqueeze(mask, dim=1), dim=0\n","            ).float()\n","            attn = (\n","                torch.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n","                + mask_float\n","            )\n","            attn = torch.reshape(attn, shape=(-1, self.num_heads, size, size))\n","            attn = nn.softmax(attn, dims=-1)\n","        else:\n","            attn = nn.softmax(attn, dims=-1)\n","        attn = self.dropout(attn)\n","\n","        x_qkv = attn @ v\n","        x_qkv = torch.permute(x_qkv, dims=(0, 2, 1, 3))\n","        x_qkv = torch.reshape(x_qkv, shape=(-1, size, channels))\n","        x_qkv = self.proj(x_qkv)\n","        x_qkv = self.dropout(x_qkv)\n","        return x_qkv"]},{"cell_type":"markdown","metadata":{"id":"Ji5mJm4pwXce"},"source":["### Swin transformer block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3G_GkSAwaEf"},"outputs":[],"source":["class SwinTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        num_patch,\n","        num_heads,\n","        window_size,\n","        shift_size,\n","        num_mlp,\n","        norm_dim,\n","        qkv_bias,\n","        dropout_rate,\n","        **kwargs,\n","    ):\n","        super(SwinTransformer, self).__init__(**kwargs)\n","\n","        self.dim = dim  # number of input dimensions\n","        self.num_patch = num_patch  # number of embedded patches\n","        self.num_heads = num_heads  # number of attention heads\n","        self.window_size = window_size  # size of window\n","        self.shift_size = shift_size  # size of window shift\n","        self.num_mlp = num_mlp  # number of MLP nodes\n","        self.norm_dim = norm_dim\n","        self.qkv_bias = qkv_bias\n","        self.dropout_rate = dropout_rate\n","\n","        self.norm1 = nn.LayerNorm(norm_dim,eps=1e-5)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=(self.window_size, self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            dropout_rate=dropout_rate,\n","        )\n","        self.drop_path = DropPath(dropout_rate)\n","        self.norm2 = nn.LayerNorm(norm_dim,eps=1e-5)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, num_mlp),\n","            nn.GELU(),\n","            nn.dropout(p=dropout_rate),\n","            nn.Linear(num_mlp, dim),\n","            nn.dropout(p=dropout_rate),\n","        )\n","\n","        if min(self.num_patch) < self.window_size:\n","            self.shift_size = 0\n","            self.window_size = min(self.num_patch)\n","\n","    def build(self, input_shape):\n","        if self.shift_size == 0:\n","            self.attn_mask = None\n","        else:\n","            height, width = self.num_patch\n","            h_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            w_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            mask_array = np.zeros((1, height, width, 1))\n","            count = 0\n","            for h in h_slices:\n","                for w in w_slices:\n","                    mask_array[:, h, w, :] = count\n","                    count += 1\n","            mask_array = torch.from_numpy(mask_array)\n","\n","            # mask array to windows\n","            mask_windows = window_partition(mask_array, self.window_size)\n","            mask_windows = torch.reshape(\n","                mask_windows, (-1, self.window_size * self.window_size)\n","            )\n","            attn_mask = torch.unsqueeze(mask_windows, axis=1) - torch.unsqueeze(\n","                mask_windows, axis=2\n","            )\n","            attn_mask = torch.where(attn_mask != 0, -100.0, attn_mask)\n","            attn_mask = torch.where(attn_mask == 0, 0.0, attn_mask)\n","            self.attn_mask = torch.Tensor(attn_mask)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, num_patches_before, channels = x.shape\n","        x_skip = x\n","        x = self.norm1(x)\n","        x = torch.reshape(x, shape=(-1, height, width, channels))\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(\n","                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n","            )\n","        else:\n","            shifted_x = x\n","\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = torch.reshape(\n","            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n","        )\n","        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n","\n","        attn_windows = torch.reshape(\n","            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n","        )\n","        shifted_x = window_reverse(\n","            attn_windows, self.window_size, height, width, channels\n","        )\n","        if self.shift_size > 0:\n","            x = torch.roll(\n","                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n","            )\n","        else:\n","            x = shifted_x\n","\n","        x = torch.reshape(x, shape=(-1, height * width, channels))\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        x_skip = x\n","        x = self.norm2(x)\n","        x = self.mlp(x)\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"iA7YQ9AgwabP"},"source":["### Extract and embed image patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFpIdzaLwd5o"},"outputs":[],"source":["patchNum = 0\n","patchDim = 0\n","embedSize = ()\n","\n","class PatchExtract(nn.Module):\n","    def __init__(self, patch_size, **kwargs):\n","        super(PatchExtract, self).__init__(**kwargs)\n","        self.patch_size_x = patch_size[0]\n","        self.patch_size_y = patch_size[0]\n","\n","    # From https://discuss.pytorch.org/t/tf-extract-image-patches-in-pytorch/43837/10\n","    def extract_image_patches(x, kernels=1, strides=1, dilation=1):\n","        # Do TF 'VALID' Padding\n","        b,c,h,w = x.shape\n","        kh, kw = kernels\n","        dh, dw = strides\n","\n","        # From user \"RoyaumeIX\" on \n","        # https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n","        h2 = math.ceil(float(h - kh + 1) / float(dh))\n","        w2 = math.ceil(float(w - kw + 1) / float(dw))\n","\n","        pad_row = (h2 - 1) * dh + (kh - 1) * dilation + 1 - h\n","        pad_col = (w2 - 1) * dw + (kw - 1) * dilation + 1 - w\n","        x = F.pad(x, (pad_row//2, pad_row - pad_row//2, pad_col//2, pad_col - pad_col//2))\n","        \n","        # Extract patches\n","        patches = x.unfold(2, kh, dh).unfold(3, kw, dw)\n","        patches = patches.permute(0,4,5,1,2,3).contiguous()\n","        \n","        return patches.view(b,-1,patches.shape[-2], patches.shape[-1])\n","\n","    def call(self, images):\n","        batch_size = images.size()[0]#tf.shape(images)[0]\n","        patches = extract_image_patches(\n","            x=images,\n","            kernels=(self.patch_size_x, self.patch_size_y),\n","            strides=(self.patch_size_x, self.patch_size_y),\n","            dilation=1,\n","        )\n","        patch_dim = patches.shape[-1]\n","        patchDim = patch_dim\n","        patch_num = patches.shape[1]\n","        patchNum = patch_num\n","        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n","\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, num_patch, patch_num, patch_dim, embed_dim, **kwargs):\n","        super(PatchEmbedding, self).__init__(**kwargs)\n","        self.num_patch = num_patch\n","        self.patch_num = patch_num\n","        self.patch_dim = patch_dim\n","        self.proj = nn.Linear(patch_num * patch_num * patch_dim, embed_dim)\n","        self.pos_embed = torch.nn.Embedding(num_embeddings=num_patch,embedding_dim=embed_dim) \n","        #layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n","\n","    def call(self, patch):\n","        pos = torch(start=0, end=self.num_patch,step=1)#tf.range(start=0, limit=self.num_patch, delta=1)\n","        result = self.proj(patch) + self.pos_embed(pos)\n","        embedSize = result.size()\n","        return result\n","\n","\n","class PatchMerging(torch.nn.Module): #(tf.keras.layers.Layer):\n","    def __init__(self, num_patch, embed_dim):\n","        super(PatchMerging, self).__init__()\n","        self.num_patch = num_patch\n","        self.embed_dim = embed_dim\n","        self.linear_trans = torch.nn.Linear(in_features = 2 * embed_dim,bias=False )#layers.Dense(2 * embed_dim, use_bias=False)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, _, C = x.get_shape().as_list()\n","        x = torch.reshape(x, shape=(-1, height, width, C))\n","        x0 = x[:, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, :]\n","        x3 = x[:, 1::2, 1::2, :]\n","        x = torch.stack((x0, x1, x2, x3),dim=1)#tf.concat((x0, x1, x2, x3), axis=-1)\n","        x = torch.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n","        return self.linear_trans(x)"]},{"cell_type":"markdown","metadata":{"id":"d9xwwMFKwfJf"},"source":["### Final model assembly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIx-4Q_mwgwZ"},"outputs":[],"source":["class finalModel(nn.Module):\n","    def __init__(\n","        self,\n","        input_shape,\n","        image_dimension,\n","        patch_size,\n","        num_patch_x,\n","        num_patch_y,\n","        patch_num,\n","        patch_dim,\n","        embed_dim,\n","        num_heads,\n","        window_size,\n","        shift_size,\n","        num_mlp,\n","        norm_dim,\n","        qkv_bias,\n","        dropout_rate,\n","        **kwargs,\n","    ):\n","        super(finalModel, self).__init__(**kwargs)\n","\n","        self.input_shape = input_shape\n","        self.image_dimension = image_dimension\n","        self.patch_size = patch_size\n","        self.num_patch_x = num_patch_x\n","        self.num_patch_y = num_patch_y\n","        self.patch_num = patch_num\n","        self.patch_dim = patch_dim\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads  # number of attention heads\n","        self.window_size = window_size  # size of window\n","        self.shift_size = shift_size  # size of window shift\n","        self.num_mlp = num_mlp  # number of MLP nodes\n","        self.norm_dim = norm_dim\n","        self.qkv_bias = qkv_bias\n","        self.dropout_rate = dropout_rate\n","\n","        self.random_crop = transforms.RandomCrop(image_dimension)\n","        self.horiz_flip = transforms.RandomHorizontalFlip()\n","        self.patch_extract = PatchExtract(patch_size)\n","        self.patch_embed = PatchEmbedding(num_patch_x * num_patch_y, patch_num, patch_dim, embed_dim)\n","        self.swin_block = SwinTransformer(\n","            dim=embed_dim,\n","            num_patch=(num_patch_x, num_patch_y),\n","            num_heads=num_heads,\n","            window_size=window_size,\n","            shift_size=0,\n","            num_mlp=num_mlp,\n","            norm_dim=norm_dim,\n","            qkv_bias=qkv_bias,\n","            dropout_rate=dropout_rate\n","        )\n","        self.patch_merge = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)\n","        self.pool = nn.AdaptiveMaxPool1d(2 * embed_dim)\n","        self.linear = nn.Linear(2 * embed_dim, num_classes)\n","\n","    def forward(self, input):\n","        input = torch.tensor(input)\n","        x = self.random_crop(input)\n","        x = self.horiz_flip(x)\n","        x = self.patch_extract(x)\n","        x = self.patch_embed(x)\n","        x = self.swin_block(x)\n","        x = self.swin_block(x)\n","        x = self.patch_merge(x)\n","        x = self.pool(x)\n","        x = self.linear(x)\n","        output = x\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"SAYNmv_CwhAh"},"source":["### Model training functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rlies82PwibJ"},"outputs":[],"source":["import torch.optim as optim\n","from ray import tune\n","\n","def train(model, data_loader, optimizer, criterion, device):\n","    model.train()\n","    print('Training')\n","    train_running_loss = 0.0\n","    train_running_correct = 0\n","    counter = 0\n","    for i, data in enumerate(data_loader):\n","        counter += 1\n","        image, labels = data\n","        image = image.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        # Forward pass.\n","        outputs = model(image)\n","        # Calculate the loss.\n","        loss = criterion(outputs, labels)\n","        train_running_loss += loss.item()\n","        # Calculate the accuracy.\n","        _, preds = torch.max(outputs.data, 1)\n","        train_running_correct += (preds == labels).sum().item()\n","        # Backpropagation.\n","        loss.backward()\n","        # Update the optimizer parameters.\n","        optimizer.step()\n","    \n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = train_running_loss / counter\n","    epoch_acc = 100. * (train_running_correct / len(data_loader.dataset))\n","    return epoch_loss, epoch_acc\n","\n","# Validation function.\n","def validate(model, data_loader, criterion, device):\n","    model.eval()\n","    print('Validation')\n","    valid_running_loss = 0.0\n","    valid_running_correct = 0\n","    counter = 0\n","    \n","    with torch.no_grad():\n","        for i, data in enumerate(data_loader):\n","            counter += 1\n","            \n","            image, labels = data\n","            image = image.to(device)\n","            labels = labels.to(device)\n","            # Forward pass.\n","            outputs = model(image)\n","            # Calculate the loss.\n","            loss = criterion(outputs, labels)\n","            valid_running_loss += loss.item()\n","            # Calculate the accuracy.\n","            _, preds = torch.max(outputs.data, 1)\n","            valid_running_correct += (preds == labels).sum().item()\n","        \n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = valid_running_loss / counter\n","    epoch_acc = 100. * (valid_running_correct / len(data_loader.dataset))\n","    return epoch_loss, epoch_acc\n","\n","\n","\n","def run_search():\n","    # Define the parameter search configuration.\n","    config = {\n","        \"first_conv_out\": \n","            tune.sample_from(lambda _: 2 ** np.random.randint(4, 8)),\n","        \"first_fc_out\": \n","            tune.sample_from(lambda _: 2 ** np.random.randint(4, 8)),\n","        \"lr\": tune.loguniform(1e-4, 1e-1),\n","        \"batch_size\": tune.choice([2, 4, 8, 16])\n","    }\n","    # Schduler to stop bad performing trails.\n","    scheduler = ASHAScheduler(\n","        metric=\"loss\",\n","        mode=\"min\",\n","        max_t=MAX_NUM_EPOCHS,\n","        grace_period=GRACE_PERIOD,\n","        reduction_factor=2)\n","    # Reporter to show on command line/output window\n","    reporter = CLIReporter(\n","        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n","    # Start run/search\n","    result = tune.run(\n","        train_and_validate,\n","        resources_per_trial={\"cpu\": CPU, \"gpu\": GPU},\n","        config=config,\n","        num_samples=NUM_SAMPLES,\n","        scheduler=scheduler,\n","        local_dir='../outputs/raytune_result',\n","        keep_checkpoints_num=1,\n","        checkpoint_score_attr='min-validation_loss',\n","        progress_reporter=reporter\n","    )\n","    # Extract the best trial run from the search.\n","    best_trial = result.get_best_trial(\n","        'loss', 'min', 'last'\n","    )\n","    print(f\"Best trial config: {best_trial.config}\")\n","    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n","    print(f\"Best trial final validation acc: {best_trial.last_result['accuracy']}\")"]},{"cell_type":"markdown","source":["###Initialize and train the model"],"metadata":{"id":"J-_Sh8XbjImQ"}},{"cell_type":"code","source":["model = finalModel(\n","    input_shape=input_shape,\n","    image_dimension=image_dimension, \n","    patch_size=patch_size,\n","    num_patch_x=num_patch_x,\n","    num_patch_y=num_patch_y,\n","    patch_num=patchNum,\n","    patch_dim=patchDim,\n","    embed_dim=embed_dim,\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=shift_size,\n","    num_mlp=num_mlp,\n","    norm_dim=embedSize,\n","    qkv_bias=True,\n","    dropout_rate=0.0,\n",")\n","\n","model_params = list(model.parameters())\n","\n","optimizer = optim.AdamW(\n","    params=model_params, \n","    lr=learning_rate, \n","    weight_decay=weight_decay\n",")\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","cuda = torch.device('cuda')"],"metadata":{"id":"AR4yPbD5jNjC","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1654277933930,"user_tz":420,"elapsed":391,"user":{"displayName":"Donald L Chan","userId":"14677021687740246131"}},"outputId":"2d2d4795-69bd-4627-98ce-a0eda7e4b811"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f141df431ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = finalModel(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_patch_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_patch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'finalModel' is not defined"]}]},{"cell_type":"code","source":["train_accuracy_log = []\n","test_accuracy_log  = []\n","\n","train_loss_log = []\n","test_loss_log  = []\n","\n","# train the model\n","for epoch in range(100):\n","    # training\n","    train_accuracy, train_loss = train(model=model,\n","                                       data_loader=trainloader,\n","                                       optimizer=optimizer,\n","                                       criterion=criterion,\n","                                       device=cuda)\n","    train_accuracy_log.append(train_accuracy)\n","    train_loss_log.append(train_loss)\n","    # inference\n","    test_accuracy, test_loss = validate(model=model,\n","                                       data_loader=testloader,\n","                                       criterion=criterion,\n","                                       device=cuda)\n","    test_accuracy_log.append(test_accuracy)\n","    test_loss_log.append(test_loss)\n","    # end\n","    print('epoch', epoch + 1,\n","          '\\ttrain accuracy:', format(train_accuracy, '.4f'),\n","          '| train loss:'    , format(train_loss    , '.4f'),\n","          '| test accuracy:' , format(test_accuracy , '.4f'),\n","          '| test loss:'     , format(test_loss     , '.4f'))\n","    \n","    if test_accuracy > 0.9 and epoch > 50:\n","        break"],"metadata":{"id":"u9rqdFaZqU31"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Swin Transformer Model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}